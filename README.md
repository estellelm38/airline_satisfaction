**L3S2 MIASHS - Introduction √† l‚Äôapprentissage automatique**

**Application d‚Äôalgorithmes de classification supervis√©e pour pr√©dire la satisfaction**

**de passagers d‚Äôune compagnie a√©rienne**

**Dossier de validation**

**LONG-MERLE Estelle | No √©tudiant: 12106173 KAHLAOUI‚ÄìGUILLAUME Rayan | No      √©tudiant: 12110555**

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.001.png)

Universit√© Grenoble Alpes

Licence 3 MIASHS

**Pr√©sentation du dataset**

Le jeu de donn√©es intitul√© ‚Äúairline passenger satisfaction‚Äù concerne la satisfaction des passagers √† bord de la compagnie a√©rienne suivante : United Airlines en classe Economy Plus. Il vise √† comprendre les facteurs qui influencent la satisfaction des passagers lors de leurs voyages en avion.

Le jeu de donn√©es est disponible sur Kaggle : https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.002.png)

Le code utilis√© pour le projet est disponible au lien github suivant : https://github.com/estellelm38/airline\_satisfaction![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.003.png)

**Data Preprocessing**

Nous cherchons √† comprendre quels facteurs influencent la satisfaction des passagers a√©riens et quelle est leur influence respective.

L ensemble de donn√©es comprend un total de 129 880 observations et 22 variables, parmi lesquelles figurent des variables cat√©gorielles, quantitatives et ordinales.

La target est le niveau de satisfaction du client de la compagnie a√©rienne dont les modalit√©s sont : satisfied / neutral or dissatisfied.

Le jeu de donn√©es est √©quilibr√©.

Nous cherchons √† obtenir la meilleure accuracy et la meilleure pr√©cision sur la target.

Les variables cat√©gorielles comprennent des informations telles que le genre, le type de client, le type de voyage et la classe de vol. Les variables quantitatives comprennent l √¢ge, la distance de vol, ainsi que les retards au d√©part et √† l arriv√©e. Les variables ordinales repr√©sentent la satisfaction des passagers √† l √©gard de diff√©rents services √† bord, tels que le wifi √† bord, la commodit√© des horaires de d√©part/arriv√©e, la r√©servation en ligne, etc.

1

Quantitatives![ref1]

Age

Flight Distance Departure Delay in Minutes

Arrival Delay in Minutes

Cat√©gorielles

Gender (Male/Female) Customer Type (Loyal/Disloyal)

Type of Travel (Personal/Business) Class (Eco plus/Business/Eco)

Ordinales

Inflight wifi service Departure/Arrival time convenient

Ease of Online booking Gate location

Food and drink

Online boarding

Seat comfort

Inflight entertainment On-board service

Leg room service Baggage handling Checkin service

Inflight service Cleanliness

2

*Figure 1 : Features du dataset airline passenger satisfaction tri√©es par type![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.005.png)*

Nous commen√ßons par observer les distributions des diff√©rentes variables √† l aide d histogrammes de countplots et de diagrammes pour mieux visualiser le jeu de donn√©es, les graphiques obtenus sont disponibles en annexe ( *Annexe : Figure 2* ).

Encodage

Les mod√®les √©tant bas√©s sur des calculs math√©matiques, il est n√©cessaire de repr√©senter les donn√©es cat√©gorielles et ordinales num√©riquement √† l aide de techniques d encodage afin qu elles soient comprises par les mod√®les.

Nous cherchons √† encoder des features, Nous allons donc utiliser les deux en- codeurs propos√©s par scikit learn dans ce cas de figure, √† savoir one hot encoder et ordinal encoder, afin d encoder respectivement les variables cat√©gorielles et ordinales.

Nous utilisons one hot encoder pour l encodage des variables cat√©gorielles afin d √©viter de cr√©er des relations d ordre entre ces variables et p√©naliser les mod√®les sensibles.

Nous allons ici d√©composer les variables en sous variables et cr√©er autant de colonnes que nous avons de cat√©gories (celles-ci sont repr√©sent√©es de fa√ßon binaires). Nous obtenons une sparse matrix tr√®s large qui est automatiquement compress√©e.

Apr√®s encodage, 6 nouvelles colonnes ont √©t√© g√©n√©r√©es (4 pour les features et 2 pour la target).

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.006.png)

*Figure 3 : variables cat√©gorielles une fois encod√©e avec le one-hot encoder* Nous utilisons l‚Äôordinal encoder pour l encodage des variables ordinales

Il permet d associer √† chaque classe une valeur num√©rique en traitant une par une

les diff√©rentes colonnes. Cela a permis de repr√©senter les diff√©rentes cat√©gories des variables ordinales par des entiers, tout en pr√©servant l ordre des cat√©gories.

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.007.png)

*Figure 4 : variables ordinales une fois encod√©e avec l‚Äôordinal encoder*

Feature scaling : Il faut mettre toutes les features √† la m√™me √©chelle (par exemple l √¢ge est de l‚Äôordre de la dizaine et la distance est de l‚Äôordre du millier et ne peuvent √™tre compar√©s entre eux tels quels) ( *Annexe : Figure 5* )

MinMax scaling : Transformer chaque feature pour que ses valeurs soient com- prises entre 0 et 1, pour cela nous avons appliqu√© la formule suivante avec chaque valeur de X pour chaque feature (X-Xmin)/(Xmax-Xmin) avec X correspondant √† la valeur d entr√©e , Xmin correspondant √† la valeur minimum et Xmax la valeur maximum de la feature.

Ce processus permet de comparer les diff√©rentes features sans perdre d‚Äôinformation car les distances entre leurs valeurs restent proportionnelles entre elles.

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.008.png)

*Figure 6 : variables quantitatives une fois misent √† l‚Äô√©chelle*

Nous avons √©galement test√© standard scaling et robust scaling cependant ceux- ci ont donn√© de moins bon r√©sultats ce qui peut √™tre expliqu√© par la grande variance des donn√©es. Pour standard scaling afin de donner un √©cart type de 1, celui-ci a produit plusieurs valeurs n√©gatives qui ont par la suite complexifi√© les calculs de certains mod√®les d apprentissage. De plus, les outliers sont rares mais importants pour le jeu de donn√©es comme on le verra juste apr√®s. Ainsi

il est moins int√©ressant d utiliser le robust scaling qui perd de l information en utilisant un calcul de m√©diane plut√¥t qu une moyenne au profit d une robustesse face aux outliers. On va ici privil√©gier au contraire une m√©thode sensible √† ces outliers.

Pour en revenir aux outliers, nous avons choisi de les garder dans le jeu de donn√©es.

En ce qui concerne les d√©lais de d√©part/arriv√©e nous avons jug√© qu‚Äôune valeur aberrante aurait une influence sur la satisfaction du passager et donc il est important de ne pas la supprimer. Ensuite nous ne supprimons pas les outliers des distances de vol parce que c est une valeur plausible (la distance maximum dans le dataset : 4983 km, est inf√©rieure √† la distance d‚Äôun vol Russie-USA : 8881 km).

Feature selection : Trouver puis s√©lectionner les variables les plus utiles pour notre mod√®le (cela permet d am√©liorer sa performance)

Nous avons trac√© la matrice des corr√©lation puis avons affich√© les features qui ont une corr√©lation |r|>0,8.

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.009.png)

*Figure 7 : paires de features corr√©l√©es √† |r|>0.8*

Nous avons ensuite supprim√© certaines valeurs pour garder une bonne compr√©hen- sion, par exemple en gardant seulement ‚Äòsatisfied‚Äô et en supprimant ‚Äòunsatisfied‚Äô qui n ajoutait pas d information suppl√©mentaire.

Nous avons supprim√© une feature (le d√©lai de d√©part en minutes) car c √©tait la seule qui comportait des valeurs manquantes et ce en grande quantit√© (393). Ainsi nous avons pr√©f√©r√© supprimer directement la cat√©gorie plut√¥t que de simuler

ces valeurs manquantes.

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.010.png)

*Figure 8 : nombres de valeurs manquantes par features*

Nous avons ensuite plot la feature\_importance ( *Annexe : Figures 9 et 10* ) qui d√©pend cette fois du mod√®le de for√™t al√©atoires en observant l importance des features apr√®s entra√Ænement du mod√®le. Finalement, nous avons par la suite

pu obtenir de bons r√©sultats m√™me en gardant toutes les features de moindre importance ainsi nous avons pr√©f√©r√© garder toutes les features pour √©viter la perte d information.

Pour la phase d apprentissage supervis√©, nous avons test√© les 4 classifieurs suivants : Le K-Nearest Neighbors (KNN), le Naive Bayes, l algorithme des for√™ts al√©atoires et le SVC ainsi que la classification par un r√©seau de neurones (MLP).

On a d√©cid√© de repr√©senter la qualit√© de l‚Äôapprentissage pour chaque mod√®le √† l‚Äôaide d‚Äôun graphique ROC c‚Äôest une mesure de la performance pour les classificateurs binaires qui donne le taux de vrais positifs en fonction du taux de faux positifs

Pour le KNN, nous avons utilis√© l estimateur KNeighborsClassifier.

Nous avons d abord divis√© notre dataset en un train set (auquel nous avons d√©cid√© d attribuer 80% des donn√©es comme cela se fait g√©n√©ralement) et un test set (avec 20% des donn√©es).

Avec le KNN, l hyper param√®tre √† tester est :

Nombre de voisins (k) : D√©termine le nombre de voisins √† prendre en compte lors de la pr√©diction.

Il faut √©galement choisir la m√©thode de calcul de la distance (distance euclidienne, distance de Manhattan, distance de minkowski).

Pour v√©rifier que notre mod√®le fonctionne correctement, on va utiliser la cross validation. Nous avons fait une KFold validation. Kfold permet de faire une v√©rification plus approfondie, plus fiable en mesurant l accuracy et la pr√©cision sur chaque Split puis en faisant la moyenne. En faisant un r√©√©chantillonnage it√©ratif sur les diff√©rentes valeurs de k de (de k=1 a k=10) on a pu trouver l hyper param√®tre optimum (ici : k=7 avec une accuracy=0.9307822605481983 et une pr√©cision= de 97%). Cela nous permet √©galement d entra√Æner et valider notre mod√®le sur diff√©rents splits possibles et d√©terminer lequel donne la meilleure performance.

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.011.png)*Figure 11 : Accuracy moyenne et erreur moyenne / standard pour une validation Kfold sur notre mod√®le KNN*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.012.png)

*Figure 12 : Pr√©cision moyenne pour une validation Kfold sur notre mod√®le KNN*

L‚Äôavantage du classifieur KNN est qu‚Äôil est plut√¥t simple √† comprendre cependant il prend en compte chaque feature et calcul toutes les distance une a une ce qui le rend extr√™mement long et sensible au bruit. Ces r√©sultats nous servent cependant de bonne base pour l analyse.

Naive Bayes:

Les classifieurs bay√©siens na√Øfs sont des mod√®les probabilistes qui cherchent √† estimer la probabilit√© d appartenance √† une classe donn√©e √©tant donn√© un vecteur de distribution. Ils reposent sur le th√©or√®me de Bayes pour calculer cette probabilit√© en se basant sur les distributions conditionnelles des features (dans notre cas : Gaussienne et Cat√©gorielle) et la probabilit√© a priori des classes.

Ces classifieurs probabilistes n‚Äôont pas d hyperparam√®tres, ils font l hypoth√®se que les features sont ind√©pendantes les unes des autres, ce qui simplifie le calcul des probabilit√©s conditionnelles. Pour estimer les param√®tres de ces distributions,

le jeu de donn√©es est souvent d√©coup√© en fonction de la classe cible, puis les moyennes et les √©carts-types (ou d autres param√®tres selon la distribution choisie) sont calcul√©s pour chaque classe.

Nous avons utilis√© les estimateurs Gaussian Naive Bayes et Categorical Naive Bayes pour traiter √† la fois les variables quantitatives et cat√©gorielles.

Apr√®s s√©paration du dataset entre train set et test set puis cross validation, le quantitatif a obtenu une accuracy moyenne d environ 68%, tandis que le cat√©goriel a obtenu une accuracy moyenne d environ 89%, sur l‚Äôensemble du dataset on obtient une accuracy totale de 82,5% et une pr√©cision de 81,6%.

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.013.png)

*Figure 14 : Accuracy et precision pour notre mod√®le de classification bay√©sien na√Øf*

Les avantages des classifieursbay√©siens na√Øfs r√©sident dans leur capacit√© √† prendre en compte des connaissances externes pour estimer les distributions, ainsi que dans leur rapidit√© de calcul. Cependant, ils pr√©sentent des limites en raison de leurs suppositions de distribution et d ind√©pendance conditionnelle.

Cet algorithme est rapide et tr√®s efficace avec un grand nombre de donn√©es donc particuli√®rement informatif sans r√©duction de dimension pr√©alable mais il ne prend pas en compte les liens entre certaines features et est connu pour √™tre un mauvais estimateur c‚Äôest pourquoi on ne retiendra pas les probabilit√©s renvoy√©es.

Random Forests:

Les for√™ts al√©atoires sont des mod√®les d apprentissage automatique qui combinent plusieurs arbres de d√©cision pour am√©liorer la pr√©cision de la pr√©diction et r√©duire le surapprentissage.

Au d√©part, plusieurs arbres de d√©cision sont entra√Æn√©s sur des sous-ensembles al√©atoires du jeu de donn√©es d entra√Ænement. Chaque arbre est form√© sur un √©chantillon de donn√©es tir√© avec remplacement (bootstrap) et sur un sous- ensemble al√©atoire des features.

Hyperparam√®tres √† tester :

Nombre d arbres de d√©cision √† inclure dans la for√™t.

Profondeur maximale des arbres : Limite la profondeur des arbres pour √©viter le surapprentissage.

Nombre minimum d √©chantillons requis pour diviser un n≈ìud en deux.

Chaque arbre de d√©cision est ensuite construit en choisissant √† chaque √©tape

le meilleur split (avec le crit√®re de gini), parmi un sous-ensemble al√©atoire des features. Cela continue jusqu √† ce que les feuilles des arbres soient "pures" ou que la profondeur maximale de l arbre soit atteinte.

Pour effectuer une pr√©diction, chaque arbre de la for√™t donne une r√©ponse bas√©e sur les features d entr√©e. Dans le cas d une classification, le r√©sultat final est d√©termin√© par un vote majoritaire parmi tous les arbres.

En combinant plusieurs arbres form√©s sur des sous-ensembles de donn√©es dif- f√©rents, les for√™ts al√©atoires r√©duisent le surapprentissage par rapport √† un seul arbre de d√©cision et augmentent la pr√©cision. Cela les rend plus robustes et moins sensibles aux variations du jeu de donn√©es d entra√Ænement.

Les for√™ts al√©atoires fournissent √©galement une mesure de l importance de chaque feature dans la pr√©diction. Cette importance est calcul√©e en mesurant la r√©duction moyenne de l impuret√© (par exemple, l indice de Gini) que chaque feature apporte lorsqu elle est utilis√©e pour diviser les arbres de la for√™t.

Nous avons utilis√© l estimateur RandomForestClassifier.

Nous avons donc s√©par√© le dataset et effectu√© une KFold validation en testant toutes les valeurs de splits.

Nous avons obtenu une accuracy d environ 96% sur l ensemble de test avec 100 estimateurs.

Les for√™ts al√©atoires sont robustes, peuvent g√©rer des donn√©es manquantes et des caract√©ristiques cat√©gorielles sans pr√©traitement suppl√©mentaire, et sont moins sensibles au surapprentissage par rapport aux arbres de d√©cision simples.

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.014.png)

*Figure 16 : Accuracy et erreur moyenne pour une validation Kfold sur notre mod√®le de for√™ts al√©atoires*

Dans l‚Äôensemble, les r√©sultats sont coh√©rents mais le classifieur le plus int√©ressant pour notre jeu de donn√©es en apprentissage supervis√© semble √™tre celui des for√™ts al√©atoires.

SVM √† noyau (appliqu√© √† la classification : SVC) :

A pour but de s√©parer les donn√©es en classes en les repr√©sentant comme des vecteurs dans un espace de caract√©ristiques. Puis on va tracer une fronti√®re de d√©cision de fa√ßon √† ce que la distance entre les classes et la fronti√®re soit maximale.

Les hyperparam√®tres √† tester sont : Le param√®tre de r√©gularisation C Le choix du noyau

Le param√®tre gamma ?

Si les donn√©es ne sont pas lin√©airement s√©parables, le SVM peut utiliser une technique appel√©e "kernel trick" pour projeter les donn√©es dans un espace de dimension sup√©rieure o√π elles peuvent √™tre s√©par√©es lin√©airement.

On prend seulement les valeurs situ√©es pr√®s de la fronti√®re entre les classes (les vecteurs support). On mesure la distance jusqu √† chacun des vecteurs support et on regarde l importance de ceux-ci mesur√©e dans le training, puis on trace la fronti√®re.

Pour faire une pr√©diction sur un nouveau point, on va projeter ces donn√©es dans l espace de caract√©ristiques et les classer en fonction du c√¥t√© de la fronti√®re o√π ils se trouvent.

Nous avons obtenu une accuracy d environ 95% et une pr√©cision de 95% sur l ensemble de test.

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.015.png)

*Figure 18 : Accuracy et erreur moyenne pour une validation Kfold sur notre mod√®le SVC*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.016.png)

*Figure 19 : Precision pour une validation Kfold sur notre mod√®le SVC*

Le SVM est capable de trouver des fronti√®res de d√©cision complexes et non lin√©aires dans des espaces de grande dimension.

Cette m√©thode est int√©ressante dans notre cas car elle permet d att√©nuer le d√©s√©quilibre du jeu de donn√©es en permettant de pond√©rer les erreurs de classifica- tion diff√©remment selon les classes. Ainsi, m√™me si une classe est sous-repr√©sent√©e, le SVM peut encore apprendre efficacement √† distinguer entre les classes.

De plus, le SVM est robuste face au surapprentissage (overfitting) et peut g√©n√©raliser facilement et rapidement √† de nouveaux exemples une fois entra√Æn√©.

R√©seau de neurones (MLP) :

Un perceptron multicouche (MLP) est un type de r√©seau de neurones artificiels

qui consiste en plusieurs couches de neurones, y compris une couche d entr√©e, une ou plusieurs couches cach√©es et une couche de sortie. Chaque neurone dans le r√©seau est connect√© √† tous les neurones des couches adjacentes, formant un r√©seau dense. Les valeurs d entr√©e sont propag√©es √† travers le r√©seau par des calculs pond√©r√©s et des transformations non lin√©aires, aboutissant √† des pr√©dictions ou

des classifications. Pendant l entra√Ænement, les poids du r√©seau sont ajust√©s par r√©tropropagation du gradient pour minimiser une fonction de perte, permettant

au mod√®le d apprendre √† partir des donn√©es.

Les hyperparam√®tres, tels que le nombre de couches cach√©es, le nombre de neurones par couche et la fonction d activation, influencent les performances et la capacit√© du mod√®le √† g√©n√©raliser.

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.017.png)

*Figure 20 : Hyperparam√®tres test√©s sur notre MLP*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.018.png)

*Figure 21 : Hyperparam√®tres pour obtenir la meilleure pr√©cision pour notre MLP*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.019.png)

*Figure 22 : Hyperparam√®tres pour obtenir la meilleure accuracy pour notre MLP* Pour l algorithme non supervis√©, on a d√©cid√© de partir sur le mod√®le CAH.

On plot un dendrogramme ( *Annexe : Figure 23* ) avec tous les individus regroup√©s en diff√©rents clusters avec toutes les √©tapes de regroupement pour chaque distance

de manhattan en utilisant le complete linkage qui vise √† regrouper les features

en clusters en fonction de leurs valeurs de distance les plus √©loign√©es, puis on choisit un seuil de distance en observant le graphique. On opte pour le seuil 36 qui nous laisse 6 clusters tout en gardant une grande distance.

PCA :

R√©duction de dimension : r√©duire la complexit√© d un dataset en projetant ses donn√©es dans un espace de plus petite dimension (avec moins de variables)

But : acc√©l√©rer l apprentissage de la machine et lutter contre le fl√©au de la dimension (risque d overfitting li√© au surplus de dimensions)

Principe PCA : projeter nos donn√©es sur des axes (composantes principales) en cherchant √† minimiser la distance entre points et projections

Ainsi, on r√©duit la dimension du dataset tout en pr√©servant au maximum la variance de nos donn√©es.

Afin de minimiser les risques d overfitting li√© au surplus de dimensions, nous avons d√©cid√© de faire une r√©duction de dimension en utilisant le traitement PCA qui va permettre de r√©duire au maximum la dimension du dataset tout en pr√©servant au maximum la variance (l information) de notre dataset.

Nous avons tout d abord calcul√© le pourcentage de variance expliqu√©e puis nous avons trac√© le graphique ( *Annexe : Figure 24* ) afin de mieux visualiser le rapport entre la variance expliqu√©e et le nombre de composantes. Le but √©tant de

d√©terminer une valeur de variance la plus √©lev√©e possible avec un minimum de composantes restantes. On remarque facilement sur le graphique obtenu qu une variance de 90% avec 10 composantes semble optimale. Ce qu on a v√©rifi√© par la suite en testant diff√©rents pourcentage de variance et en comparant la pr√©cision obtenue apr√®s apprentissage supervis√© sans l √©tape de r√©duction de dimension. Une observation qui saute aux yeux est la diff√©rence de vitesse de calcul qui est extr√™mement plus rapide avec un traitement PCA pr√©alable

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.020.png)

*Figure 25 : Accuracy et erreur moyenne pour une validation Kfold sur notre mod√®le KNN avec PCA*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.021.png)

*Figure 26 : Pr√©cision pour une validation Kfold sur notre mod√®le KNN avec PCA*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.022.png)

*Figure 28 : Accuracy et precision pour notre mod√®le de classification bay√©sien na√Øf avec PCA*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.023.png)

*Figure 30 : Accuracy en train et en test pour notre for√™t al√©atoire avec PCA* Voici un r√©sum√© des classifieurs test√©s :![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.024.png)

Classifieurs Accuracy Pr√©cision Classement Classement

Accuracy Pr√©cision KNN 0.93 0.97 3 2

Na√Øf Bay√©sien 0.82 0.82 6![ref1]

For√™ts 0.96 **1**

Al√©atoires

SVC 0.95 0.95 2 4 MLP 0.93 0.98 3 **1** Avec PCA

KNN 0.92 0.96 4 3 Na√Øf Bay√©sien 0.83 0.84 5 5 For√™ts 0.93 3

Al√©atoires![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.025.png)

Dans l √©tude comparative des performances des mod√®les de classificationappliqu√©s

√† notre ensemble de donn√©es, les r√©sultats d√©montrent que le mod√®le de for√™ts al√©atoires offrela meilleure pr√©cision globale, mesur√©e par l accuracy. En revanche, le perceptron multicouche (MLP) se distingue par sa capacit√© √† produire la meilleure pr√©cision, en particulier dans la pr√©diction des classes positives, comme

l illustre le score de pr√©cision. Ces conclusions sont d autant plus significatives lorsqu on les compare aux r√©sultats ant√©rieurs disponibles sur Kaggle, mettant en √©vidence l am√©lioration de la performance des mod√®les sur notre jeu de donn√©es sp√©cifique.

On pourrait poursuivre ce projet en aboutissant l‚Äôapprentissage non supervis√© notamment en testant la m√©thode kmeans et en faisant l‚Äô√©valuation de nos clusters avec CAH.

**Annexe![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.026.png)**

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.027.png)

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.028.png)

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.029.png)

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.030.png)

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.031.png)

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.032.png)

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.033.png)

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.034.png)

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.035.png)

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.036.png)

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.037.png)

*Figure 2 : r√©partition des individus par features dans le dataset![ref1]![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.038.png)*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.039.png)

*Figure 5 : description des statistiques univari√©es pour l‚Äô√¢ge et la distance de vol*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.040.jpeg)

*Figure 9 : Importance de chaque feature calcul√©e par le classifieur for√™ts al√©atoires*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.041.jpeg)

*Figure 10 : Importances cumul√©es des features selon le classifieur for√™ts al√©atoires avec un seuil √† 80%*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.042.jpeg)

*Figure 13 : Graphique ROC pour KNN √©valu√© par Kfold*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.043.jpeg)

*Figure 15 : Graphique ROC pour classifieur bay√©sien na√Øf √©valu√© par Kfold*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.044.jpeg)

*Figure 17 : Graphique ROC pour for√™ts al√©atoires √©valu√©es par Kfold*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.045.jpeg)

*Figure 23 : Dendrogramme g√©n√©r√© par CAH*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.046.jpeg)

*Figure 24 : Diagramme du coude pour la PCA avec un seuil 0.95*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.047.jpeg)

*Figure 27 : Graphique ROC pour KNN avec PCA √©valu√© par Kfold*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.048.jpeg)

*Figure 29 : Graphique ROC pour classifieur bay√©sien na√Øf avec PCA √©valu√© par*

*Kfold*

![](Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.049.jpeg)

*Figure 29 : Graphique ROC pour for√™ts al√©atoires avec PCA √©valu√©es par Kfold*
23

[ref1]: Aspose.Words.64839b93-4eac-4803-95cb-7288c9b3cbec.004.png
